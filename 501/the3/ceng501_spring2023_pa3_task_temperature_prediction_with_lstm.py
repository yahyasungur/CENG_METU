# -*- coding: utf-8 -*-
"""Copy of CENG501 - Spring2023 - PA3 - Task - temperature prediction with LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nw1kbcRaWAiEeaUoJ1DeN9maM5DXg5dP

# A Sequence Modeling Pipeline with PyTorch for Weather Prediction
# CENG501 - Spring 2023 - PA3

In this task, you are to follow a pipeline for training a recurrent network.

## 1 Import Modules
"""

# Import your modules here
import random
import numpy as np
import pandas as pd


"""## 2 Dataset

We will use the hourly temperature data for Basel (why Basel? because it was freely available) from [meteoblue](https://www.meteoblue.com/en/weather/archive/export/basel_switzerland_2661604?daterange=2021-06-01%20-%202021-06-23&domain=NEMSAUTO&params%5B%5D=temp2m&min=2021-06-16&max=2021-06-23&utc_offset=2&timeResolution=hourly&temperatureunit=CELSIUS&velocityunit=KILOMETER_PER_HOUR&energyunit=watts&lengthunit=metric&degree_day_type=10%3B30&gddBase=10&gddLimit=30) between 1 June 2021 and 23 June 2021. Here is a description of how you should prepare your dataset from this raw file:

* Raw data is available at [https://ceng.metu.edu.tr/~skalkan/DL/Basel_weather.xlsx](https://ceng.metu.edu.tr/~skalkan/DL/Basel_weather.xlsx) as an XLSX file which was downloaded from [meteoblue](https://www.meteoblue.com/en/weather/archive/export/basel_switzerland_2661604?daterange=2021-06-01%20-%202021-06-23&domain=NEMSAUTO&params%5B%5D=temp2m&min=2021-06-16&max=2021-06-23&utc_offset=2&timeResolution=hourly&temperatureunit=CELSIUS&velocityunit=KILOMETER_PER_HOUR&energyunit=watts&lengthunit=metric&degree_day_type=10%3B30&gddBase=10&gddLimit=30).

* Load the file into Python as a NumPy array using [Pandas](https://pp4e-book.github.io/chapters/ch10_scientific_libraries.html#data-handling-analysis-with-pandas). Filter out unnecessary information at the top and convert the first column to an hour index, starting at 0 and finishing at 551.

* Split the data into two: A training set from the values between 1 June 2021 and 16 June 2021 (inclusive). A test set from the remaining values.

* For each set, slide a time window of six hours and with a stride of 1, and for each position of the window, create a training input-output pair as follows:

  $(\mathbf{x}_i = <T_i, T_{i+1}, T_{i+2}, T_{i+3}, T_{i+4}>, y_i = T_{i+5}),$

  where $T_i$ is the temperature at hour index $i$.

"""

# Write your code here
# If you like, you can create subsections here and split your code into
# meaningfully separate parts, e.g. "Loading the dataset", "Cleaning the dataset",
# "Splitting the dataset"..
# LOAD THE DATA

# Load the data from https://ceng.metu.edu.tr/~skalkan/DL/Basel_weather.xlsx as a NumPy array using Pandas
data = pd.read_excel('https://ceng.metu.edu.tr/~skalkan/DL/Basel_weather.xlsx', header=3)
data = data.to_numpy()
print(data[:6])
# Filter out unnecessary information at the top
data = data[6:]

# Convert the first column to an hour index, starting at 0 and finishing at 551.
data[:, 0] = np.arange(0, 552)
print(data[:6])

# Split the data into two: A training set from the values between 1 June 2021 and 16 June 2021 (inclusive). A test set from the remaining values.
train_data = data[:360]
test_data = data[360:]
print(train_data.shape)
print(test_data.shape)

# For each set, slide a time window of six hours and with a stride of 1, and for each position of the window, create a training input-output pair
# as follows: (xi=<Ti,Ti+1,Ti+2,Ti+3,Ti+4>,yi=Ti+5)
# where Ti is the temperature at hour index i.
def create_dataset(data, window_size):
    X, y = [], []
    for i in range(len(data)-window_size):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size])
    return np.array(X), np.array(y)

window_size = 6
X_train, y_train = create_dataset(train_data[:, 1], window_size)
X_test, y_test = create_dataset(test_data[:, 1], window_size)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)



"""## 3 Your LSTM Definition

Implement your own LSTM "cell" using PyTorch without using PyTorch's LSTM implementation. However, you can use the backpropagation (gradient computation) mechanism of PyTorch and therefore, you just need to worry about feedforward processing.

Your implementation should not be limited to the weather prediction problem and be general. For the sake of simplicity, you can just implement a single-layer LSTM cell.
"""

# Your LSTM Definition
import torch
class MyLSTM(torch.nn.Module):
    def __init__(self, input_size, hidden_size):
        """
          input_size: the size of the input at a time step.
          hidden_size: the number of neurons in the hidden state.
        """
        super().__init__()

        # Create parameters in LSTM and initialize them
        self.W_f = torch.nn.Parameter(torch.Tensor(input_size, hidden_size))
        self.U_f = torch.nn.Parameter(torch.Tensor(hidden_size, hidden_size))
        self.b_f = torch.nn.Parameter(torch.Tensor(hidden_size))

    def forward(self, X):
        """
          X: An input that has L time steps and for each time step, it has
          input_size many elements. Has shape (B, L, input_size) with B being
          the batch size.

          Output: Tuple (h, c) where h is the tensor holding the hidden state for L
          time steps, and c is the tensor holding the memory state for L time steps.
          Both have shape (B, L, hidden_size).
        """
        # Process X over L timesteps and return the output
        # Initialize h and c
        h = torch.zeros(X.shape[0], X.shape[1], self.W_f.shape[1])
        c = torch.zeros(X.shape[0], X.shape[1], self.W_f.shape[1])
        for i in range(X.shape[1]):
            # Compute the forget gate
            f = torch.sigmoid(torch.matmul(X[:, i], self.W_f) + torch.matmul(h[:, i-1], self.U_f) + self.b_f)
            # Compute the hidden state
            h[:, i] = f * h[:, i-1]
            # Compute the memory state
            c[:, i] = f * c[:, i-1]
        return h, c

"""## 4 Your Sequence Model for Weather Prediction"""

class MyWeatherPredictor(torch.nn.Module):

    def __init__(self, input_dim, hidden_dim):
        super().__init__()

        random.seed(501)
        np.random.seed(501)
        torch.manual_seed(501)

        # Create an instance of your LSTM model and a FC layer
        # that maps the last hidden state to the output that you wish to
        # estimate
        self.lstm = MyLSTM(input_dim, hidden_dim)
        self.fc = torch.nn.Linear(hidden_dim, 1)

    
    def forward(self, X):
        # Forward pass through LSTM and FC layer to estimate
        # the target
        h, c = self.lstm(X)
        prediction = self.fc(h[:, -1])

        return prediction

"""## 5 Your Trainer

Implement your training function here. You can use functions we have defined in the previous assignments.
"""

# Your implementation comes here
# Define a function that trains the model for one epoch
def train(model, criterion, optimizer, epochs, dataloader, verbose=True):
  """
    Define the trainer function. We can use this for training any model.
    The parameter names are self-explanatory.

    Returns: the loss history.
  """
  loss_history = [] 
  for epoch in range(epochs):
    for i, data in enumerate(dataloader, 0):    
      
      # Our batch:
      inputs, labels = data
      inputs = inputs.to(device)
      labels = labels.to(device)

      # zero the gradients as PyTorch accumulates them
      optimizer.zero_grad()

      # Obtain the scores
      outputs = model(inputs)

      # Calculate loss
      loss = criterion(outputs.to(device), labels)

      # Backpropagate
      loss.backward()

      # Update the weights
      optimizer.step()

      loss_history.append(loss.item())
    
    if verbose: print(f'Epoch {epoch} / {epochs}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5}')

  return loss_history

"""## 6 Train Your Model

Create an instance of your model, a suitable loss function, a suitable optimizer and call the training function with suitable hyperparameters (learning rate, batch size, hidden size etc.).
"""

# Your implementation comes here
# Create an instance of your model
model = MyWeatherPredictor(1, 10)
model.to(torch.device)

# Create an instance of your loss function
criterion = torch.nn.MSELoss()

# Create an instance of your optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

train_dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train)), batch_size=32, shuffle=True)

epochs = 10

# Train the model
loss_history = train(model, criterion, optimizer, epochs, train_dataloader)


"""## 7 Analyze the Results

### 7.1 Visualize the Loss Curve
"""

# Your implementation comes here

pass

"""### 7.2 Quantitative Analysis

Provide a quantitative analysis of your model on the test set using root mean squared error.
"""

# Your implementation comes here

pass

"""### 7.3 Qualitative Analysis

Plot test data and your predictions over a sliding window. Plot two versions of your predictions:

(i) Slide a window on the test data, and plot the predictions only.

(ii) Use only the first window of the test data, let's denote this with $<T_0, T_{1}, T_{2}, T_{3}, T_{4}>$. After obtaining the first prediction $T'_{5}$ on this window, make your second prediction by sliding the window to include the new prediction, i.e., taking as input $<T_{1}, T_{2}, T_{3}, T_{4}, T_{5}'>$. Continue in this fashion to obtain the same number of predictions as the size of the test data.
"""

# Your implementation comes here

pass

"""## 8 Tune Your Model

Tune the following aspects for your model and provide a figure or a table in each case:

- Number of hidden neurons.
- Learning rate.
- Batch size.

Report the performance of the best model after tuning.
"""